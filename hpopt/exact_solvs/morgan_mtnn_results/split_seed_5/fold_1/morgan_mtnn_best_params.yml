activation_fn: ReLU
dropout_rate: 0.13507602848769823
learning_rate: 0.00398028718979308
n_shared_layers: 1
shared_layer_size_0: 1024
