activation_fn: ReLU
dropout_rate: 0.0037746939159024467
learning_rate: 0.0003136668335828769
n_shared_layers: 1
shared_layer_size_0: 512
